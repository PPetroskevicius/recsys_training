{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 7: Content-based Filtering for Rating Prediction using a Factorization Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we switch from collaborative to content-based filtering. Where collaborative filtering exploits similarities among interactions, content-based filtering exploits similarities between user and/or item features. It finds combinations of user-item features that help to predict ratings or rankings.\n",
    "\n",
    "However, we discussed the superiority of the ranking approach before, for simplicity we do rating prediction again here. The rating predictions are hence used to impose an ordering on items that are then recommended to the user.\n",
    "\n",
    "The model we use for the relationship between features and ratings is a factorization machine which is similar to matrix factorization and offers more flexibility in modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import itertools\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyfm import pylibfm\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_training.data import Dataset, genres, get_user_profiles\n",
    "from recsys_training.evaluation import get_relevant_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml100k_ratings_filepath = '../data/raw/ml-100k/u.data'\n",
    "ml100k_item_filepath = '../data/raw/ml-100k/u.item'\n",
    "ml100k_user_filepath = '../data/raw/ml-100k/u.user'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset(ml100k_ratings_filepath)\n",
    "data.rating_split(seed=42)\n",
    "user_ratings = data.get_user_ratings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feat = pd.read_csv(ml100k_item_filepath, sep='|', header=None,\n",
    "                        names=['item', 'title', 'release', 'video_release', 'imdb_url']+genres,\n",
    "                        engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feat = pd.read_csv(ml100k_user_filepath, sep='|', header=None,\n",
    "                        names=['user', 'age', 'gender', 'occupation', 'zip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User and Item Content (Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep the following information for items:\n",
    "* release year\n",
    "* genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(val, bounds):\n",
    "    min_max_range = bounds['max']-bounds['min']\n",
    "    return (val-bounds['min'])/min_max_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer the release year\n",
    "idxs = item_feat[item_feat['release'].notnull()].index\n",
    "item_feat.loc[idxs, 'release_year'] = item_feat.loc[idxs, 'release'].str.split('-')\n",
    "item_feat.loc[idxs, 'release_year'] = item_feat.loc[idxs, 'release_year'].apply(lambda val: val[2]).astype(int)\n",
    "\n",
    "# Impute median release year value for the items with missing release year\n",
    "top_year = item_feat.loc[idxs, 'release_year'].astype(int).describe()['50%']\n",
    "idx = item_feat[item_feat['release'].isnull()].index\n",
    "item_feat.loc[idx, 'release_year'] = top_year\n",
    "\n",
    "# Min-max scale the release year\n",
    "item_year_bounds = {'min': item_feat['release_year'].min(),\n",
    "                    'max': item_feat['release_year'].max()}\n",
    "item_feat['release_year'] = item_feat['release_year'].apply(\n",
    "    lambda year: min_max_scale(year, item_year_bounds))\n",
    "\n",
    "# Drop other columns\n",
    "item_feat.drop(['title', 'release', 'video_release', 'imdb_url'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep the following information for users:\n",
    "* age\n",
    "* gender\n",
    "* occupation\n",
    "* zip-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-max scale the age\n",
    "user_age_bounds = {'min': user_feat['age'].min(),\n",
    "                   'max': user_feat['age'].max()}\n",
    "user_feat['age'] = user_feat['age'].apply(lambda age: min_max_scale(age, user_age_bounds))\n",
    "\n",
    "# Transform gender characters to numerical values (categories)\n",
    "genders = sorted(user_feat['gender'].unique())\n",
    "user_gender_map = dict(zip(genders, range(len(genders))))\n",
    "user_feat['gender'] = user_feat['gender'].map(user_gender_map)\n",
    "\n",
    "# Transform occupation strings to numerical values (categories)\n",
    "occupations = sorted(user_feat['occupation'].unique())\n",
    "user_occupation_map = dict(zip(occupations, range(len(occupations))))\n",
    "user_feat['occupation'] = user_feat['occupation'].map(user_occupation_map)\n",
    "\n",
    "# Transform the zip codes to categories keeping the first three digits and impute for missing\n",
    "idxs = user_feat[~user_feat['zip'].str.isnumeric()].index\n",
    "user_feat.loc[idxs, 'zip'] = '00000'\n",
    "zip_digits_to_cut = 3\n",
    "user_feat['zip'] = user_feat['zip'].apply(lambda val: int(val) // 10 ** zip_digits_to_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Parrot.png)\n",
    "\n",
    "**TODO:** Complete `get_user_profiles` to infer user profiles combining their ratings with the item features the users liked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we infer profiles by combining item information with rating data for each user to get features that represent the users' preferred genres and film age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_profiler(group):\n",
    "    genre_dist = group[genres].mean()\n",
    "    year_dist = group['release_year'].describe()[['mean', 'std', '50%']]\n",
    "\n",
    "    return pd.concat((genre_dist, year_dist), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_profiles(ratings: pd.DataFrame,\n",
    "                      item_feat: pd.DataFrame,\n",
    "                      min_rating: float = 4.0) -> pd.DataFrame:\n",
    "    pass\n",
    "    \n",
    "    return profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we join the original user information with their profiles' information and one-hot-encode categorical information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = get_user_profiles(data.train_ratings, item_feat)\n",
    "user_feat = user_feat.merge(profiles, on='user', how='left')\n",
    "\n",
    "occupation_1H = pd.get_dummies(user_feat['occupation'], prefix='occupation')\n",
    "zip_1H = pd.get_dummies(user_feat['zip'], prefix='zip')\n",
    "\n",
    "user_feat.drop(['occupation', 'zip', ], axis=1, inplace=True)\n",
    "user_feat = pd.concat([user_feat, occupation_1H, zip_1H], axis=1)\n",
    "\n",
    "user_feat.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the user/item id columns and replace the current dataframe indices with their values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feat.index = user_feat['user'].values\n",
    "user_feat.drop('user', axis=1, inplace=True)\n",
    "\n",
    "item_feat.index = item_feat['item'].values\n",
    "item_feat.drop('item', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparsity of user/item content information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(user_feat==0).sum().sum()/user_feat.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(item_feat==0).sum().sum()/item_feat.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorization Machine for a Content-based Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FM](fm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Steffen Rendle: Factorization Machines](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)\n",
    "\n",
    "[pyFM - Factorization Machines in Python](https://github.com/coreylynch/pyFM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Feature Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch content information for all observed user-item rating combinations\n",
    "user_cb_feat_train = user_feat.loc[data.train_ratings.user.values].values\n",
    "user_cb_feat_test = user_feat.loc[data.test_ratings.user.values].values\n",
    "item_cb_feat_train = item_feat.loc[data.train_ratings.item.values].values\n",
    "item_cb_feat_test = item_feat.loc[data.test_ratings.item.values].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate user and item content information to form design matrices\n",
    "# and convert to sparse matrix in Compressed Sparse Row (CSR) format\n",
    "X_train = np.concatenate((user_cb_feat_train, item_cb_feat_train), axis=1)\n",
    "X_train = sparse.csr_matrix(X_train)\n",
    "X_test = np.concatenate((user_cb_feat_test, item_cb_feat_test), axis=1)\n",
    "X_test = sparse.csr_matrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity(sparse_arr) -> float:\n",
    "    num_elements = sparse_arr.shape[0]*sparse_arr.shape[1]\n",
    "    num_nonzero_elements = sparse_arr.nnz\n",
    "    density = num_nonzero_elements/num_elements\n",
    "    return 1-density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsity of Training Data\n",
    "get_sparsity(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsity of Test Data\n",
    "get_sparsity(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Target Matrices for Rating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data.train_ratings.rating.values.astype(float)\n",
    "y_test = data.test_ratings.rating.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Factorization Machine for Rating Prediction as Regressor using pyFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10  # number of full stochastic passes through the training data\n",
    "k = 16\n",
    "random_seed = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_cb = pylibfm.FM(num_factors=k,\n",
    "                   num_iter=n_epochs,\n",
    "                   verbose=True,\n",
    "                   task=\"regression\",\n",
    "                   initial_learning_rate=0.001,\n",
    "                   learning_rate_schedule=\"optimal\",\n",
    "                   seed=random_seed)\n",
    "fm_cb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fm_cb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MSE$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MAE$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(fm: object, user: int, user_feat: pd.DataFrame, item_feat: pd.DataFrame,\n",
    "                   items: np.array = None, remove_known_pos: bool = True) -> Dict[int, Dict[str, float]]:\n",
    "    \n",
    "    if items is None:\n",
    "        if remove_known_pos:\n",
    "            # Predict from unobserved items\n",
    "            known_items = np.array(list(user_ratings[user].keys()))\n",
    "            items = np.setdiff1d(data.items, known_items)\n",
    "        else:\n",
    "            items = np.array(data.items)\n",
    "    if type(items) == np.int64:\n",
    "        items = np.array([items])\n",
    "    \n",
    "    n_items = len(items)\n",
    "    \n",
    "    single_user_cb_feat = user_feat.loc[user].values.reshape(1, -1).repeat(n_items, axis=0)\n",
    "    all_items_cb_feat = item_feat.loc[items].values\n",
    "    \n",
    "    input_data = np.concatenate((single_user_cb_feat, all_items_cb_feat), axis=1)\n",
    "    input_data = sparse.csr_matrix(input_data)\n",
    "    \n",
    "    preds = fm.predict(input_data)\n",
    "    sorting = np.argsort(preds)[::-1]\n",
    "    \n",
    "    preds = {item: {'pred': pred} for item, pred in\n",
    "             zip(items[sorting], preds[sorting])}\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = get_prediction(fm_cb, 1, user_feat, item_feat)\n",
    "list(predictions.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(fm_cb: object, user: int, N: int, user_feat: pd.DataFrame, item_feat: pd.DataFrame,\n",
    "                        remove_known_pos: bool = True) -> List[Tuple[int, Dict[str, float]]]:\n",
    "    predictions = get_prediction(fm_cb, user, user_feat, item_feat,\n",
    "                                 remove_known_pos=remove_known_pos)\n",
    "    recommendations = []\n",
    "    # TODO: Simplify\n",
    "    for item, pred in predictions.items():\n",
    "        add_item = (item, pred)\n",
    "        recommendations.append(add_item)\n",
    "        if len(recommendations) == N:\n",
    "            break\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations(fm_cb, 1, N=10, user_feat=user_feat, item_feat=item_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_items = get_relevant_items(data.test_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = relevant_items.keys()\n",
    "prec_at_N = dict.fromkeys(data.users)\n",
    "\n",
    "for user in users:\n",
    "    recommendations = get_recommendations(fm_cb, user, N, user_feat=user_feat, item_feat=item_feat, remove_known_pos=True)\n",
    "    recommendations = [val[0] for val in recommendations]\n",
    "    hits = np.intersect1d(recommendations,\n",
    "                          relevant_items[user])\n",
    "    prec_at_N[user] = len(hits)/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([val for val in prec_at_N.values() if val is not None])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
