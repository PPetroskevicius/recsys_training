{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 5: Model-based Collaborative Filtering for **Rating** Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this unit, we change the approach towards CF from neighborhood-based to **model-based**. This means that we create and train a model for describing users and items instead of using the k nearest neighbors. The model parameters are latent representations for users and items.\n",
    "\n",
    "Key to this idea is to compress the sparse interaction information of $R$ by finding two matrices $U$ and $V$ that by multiplication reconstruct $R$. The decomposition of $R$ into $U \\times V$ is called _matrix factorization_ and we refer to $U$ as user latent factor matrix and $V$ as item latent factor matrix. In addition to the dot product of user and item latent factors we also model user- and item-specific biases. This reflects the intuitive understanding of some users being more critical and some items being systematically rated better or worse, e.g. due to popularity. Among these biases, we also estimate the global mean $\\mu$ as the overall average rating.\n",
    "\n",
    "Compressing the sparse matrix into the product of two matrices means that the two remaining matrices are much smaller. This decrease in size is governed by the dimension of latent user/item vectors and symbolized by $d \\in \\mathbb{N}$. We choose $d$ to be much smaller than the number of items or users:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\underset{m\\times n}{\\mathrm{R}} \\approx  \\underset{m\\times d}{U} \\times \\underset{d\\times n}{V^T} \\\\\n",
    "dÂ \\ll \\min\\{m, n\\} \\\\\n",
    "\\hat{r}_{ij} = \\mu + b_i + b_j + u_i^{T} \\cdot v_j \\\\\n",
    "\\mu, b_i, b_j \\in \\mathbb{R}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import itertools\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_training.data import Dataset\n",
    "from recsys_training.evaluation import get_relevant_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml100k_ratings_filepath = '../data/raw/ml-100k/u.data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 49\n",
    "d = 40\n",
    "model_biases = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset(ml100k_ratings_filepath)\n",
    "data.rating_split(seed=seed)\n",
    "user_ratings = data.get_user_ratings()\n",
    "\n",
    "m = data.n_users\n",
    "n = data.n_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the user and item latent factors, i.e. the model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we want to learn the user/item latent factors from rating data, we first randomly initialize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "user_factors = np.random.normal(0, .1, (m, d))\n",
    "item_factors = np.random.normal(0, .1, (n, d))\n",
    "\n",
    "if model_biases:\n",
    "    user_biases = np.random.normal(0, .1, (m, 1))\n",
    "    item_biases = np.random.normal(0, .1, (n, 1))\n",
    "else:\n",
    "    user_biases = None\n",
    "    item_biases = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = data.train_ratings[['user', 'item', 'rating']].sample(frac=1, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5304375"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_mean = ratings[\"rating\"].mean()\n",
    "global_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$u_1^{T} \\cdot v_{223}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.12336546162571498"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(user_factors[1], item_factors[233])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_variables = {\n",
    "    \"user_factors\": user_factors,\n",
    "    \"item_factors\": item_factors,\n",
    "    \"user_biases\": user_biases,\n",
    "    \"item_biases\": item_biases,\n",
    "    \"global_mean\": global_mean\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and Gradient Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Parrot.png)\n",
    "\n",
    "**Task:** Implement a function `predict` that takes as input the latent factors for users, items and the biases to return the corresponding rating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_variables: dict, user_idxs: np.array, item_idxs: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    return Bx1 array where B is the number of ratings to be predicted\n",
    "    \"\"\"\n",
    "    user_factors = model_variables[\"user_factors\"][user_idxs]\n",
    "    item_factors = model_variables[\"item_factors\"][item_idxs]\n",
    "    \n",
    "    r_hat = None\n",
    "    \n",
    "    if model_variables[\"user_biases\"] is not None:\n",
    "        pass\n",
    "    if model_variables[\"item_biases\"] is not None:\n",
    "        pass\n",
    "    if model_variables[\"global_mean\"] is not None:\n",
    "        r_hat += global_mean\n",
    "    \n",
    "    return r_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Parrot.png)\n",
    "\n",
    "**Task:** Implement `compute_gradients` that receives a minibatch and computes the gradients for user and item latent vectors involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(error: np.array,\n",
    "                      user_idxs: np.array,\n",
    "                      item_idxs: np.array,\n",
    "                      model_variables: dict,\n",
    "                      reg_rate: float = 0.02) -> Dict[str, np.array]:\n",
    "    \n",
    "    user_factors = model_variables[\"user_factors\"][user_idxs]\n",
    "    item_factors = model_variables[\"item_factors\"][item_idxs]\n",
    "    \n",
    "    gradients = {}\n",
    "    gradients[\"user_factors\"] = None\n",
    "    gradients[\"item_factors\"] = None\n",
    "    \n",
    "    if model_variables[\"user_biases\"] is not None:\n",
    "        gradients[\"user_biases\"] = None\n",
    "    \n",
    "    if model_variables[\"item_biases\"] is not None:\n",
    "        gradients[\"item_biases\"] = None\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model to the data with a technique called _minibatch gradient descent_.\n",
    "\n",
    "This means that for a number of epochs, i.e. full passes through the training data (ratings), we randomly choose a small subset of ratings (our minibatch) holding user, item and rating for each instance. Then, we compute the rating prediction as the dot product of user and item latent vectors (also called embeddings) and compute the mean squared error between predicted and true rating. We derive this error for user and item latent vectors to obtain our partial derivatives. We subtract part of the gradient from our latent vectors to move into the direction of minimizing error, i.e. deviation between true values and predictions.\n",
    "\n",
    "To keep track of the decreasing error, we compute the root mean squared error and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "reg_rate = 0.01\n",
    "\n",
    "num_batches = int(np.ceil(len(ratings) / batch_size))\n",
    "train_rmse_trace = []\n",
    "test_rmse_trace = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for idx in range(num_batches):\n",
    "        \n",
    "        minibatch = ratings.iloc[idx * batch_size:(idx + 1) * batch_size]\n",
    "        \n",
    "        user_idxs = (minibatch[\"user\"].values - 1)\n",
    "        item_idxs = (minibatch[\"item\"].values - 1)\n",
    "        \n",
    "        # Generate Rating Predictions\n",
    "        preds = predict(model_variables, user_idxs, item_idxs)\n",
    "        \n",
    "        # Compute Error\n",
    "        error = (minibatch['rating'].values - preds).reshape(-1, 1)\n",
    "        \n",
    "        # Compute Gradients\n",
    "        gradients = compute_gradients(error,\n",
    "                                      user_idxs,\n",
    "                                      item_idxs,\n",
    "                                      model_variables,\n",
    "                                      reg_rate=reg_rate)\n",
    "        \n",
    "        # Perform Update Step, i.e. apply gradients to model variables\n",
    "        model_variables[\"user_factors\"][user_idxs] += learning_rate*gradients[\"user_factors\"]\n",
    "        model_variables[\"item_factors\"][item_idxs] += learning_rate*gradients[\"item_factors\"]\n",
    "        if model_biases:\n",
    "            model_variables[\"user_biases\"][user_idxs] += learning_rate*gradients[\"user_biases\"]\n",
    "            model_variables[\"item_biases\"][item_idxs] += learning_rate*gradients[\"item_biases\"]\n",
    "        \n",
    "        if not idx % 300:\n",
    "            train_rmse = np.sqrt(np.mean(error ** 2))\n",
    "            \n",
    "            user_idxs = (data.test_ratings['user'].values - 1)\n",
    "            item_idxs = (data.test_ratings['item'].values - 1)\n",
    "            \n",
    "            test_predictions = predict(model_variables, user_idxs, item_idxs)\n",
    "            \n",
    "            test_error = (data.test_ratings['rating'].values - test_predictions).reshape(-1, 1)\n",
    "            test_rmse = np.sqrt(np.mean(test_error ** 2))\n",
    "            \n",
    "            train_rmse_trace.append(train_rmse)\n",
    "            test_rmse_trace.append(test_rmse)\n",
    "            print(f\"Epoch: {epoch:02d} - Batch: {idx:04d}, Train RMSE: {train_rmse:.3f}, Test RMSE: {test_rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(range(len(train_rmse_trace)), train_rmse_trace, 'b--', label='Train')\n",
    "plt.plot(range(len(test_rmse_trace)), test_rmse_trace, 'g--', label='Test')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the model for Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now created a model to describe users and items in terms of latent vectors. We fitted them to reconstruct ratings by multiplication. So for obtaining recommendations we simply multiply user-item latent vectors we are interested in and see favorable combinations where predicted ratings, i.e. the products, are rather high.\n",
    "\n",
    "Thus, before writing the `get_recommendations` we first implement `get_prediction`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Parrot.png)\n",
    "\n",
    "**Task:** Implement `get_prediction` for predicting ratings for a user and all items or a set of provided items. Remember to remove _known positives_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(user: int,\n",
    "                   model_variables: dict,\n",
    "                   user_ratings: Dict[int, Dict[int, float]] = user_ratings,\n",
    "                   items: np.array = None,\n",
    "                   data: object = data,\n",
    "                   remove_known_pos: bool = True) -> Dict[int, Dict[str, float]]:\n",
    "    if items is None:\n",
    "        if remove_known_pos:\n",
    "            # Predict from unobserved items\n",
    "            known_items = np.array(list(user_ratings[user].keys()))\n",
    "            items = np.setdiff1d(data.items, known_items)\n",
    "        else:\n",
    "            items = np.array(data.items)\n",
    "    if type(items) == np.int64:\n",
    "        items = np.array([items])\n",
    "    \n",
    "    user_embed = user_factors[user - 1].reshape(1, -1)\n",
    "    item_embeds = item_factors[items - 1].reshape(len(items), -1)\n",
    "    \n",
    "    # use array-broadcasting\n",
    "    item_idxs = (items - 1)\n",
    "    \n",
    "    #\n",
    "    # Implement the computation of predictions here\n",
    "    #\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_predictions = get_prediction(1, model_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(item_predictions.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected Output for top 10 ratings and user 1:\n",
    "\n",
    "```\n",
    "[(408, {'pred': 5.2480594734673085}),\n",
    " (96, {'pred': 5.119165625803175}),\n",
    " (276, {'pred': 5.115088111664335}),\n",
    " (483, {'pred': 4.915750082559056}),\n",
    " (515, {'pred': 4.864894261892342}),\n",
    " (603, {'pred': 4.817879660034034}),\n",
    " (126, {'pred': 4.758454694246811}),\n",
    " (513, {'pred': 4.748786974584198}),\n",
    " (484, {'pred': 4.710440872774363}),\n",
    " (963, {'pred': 4.7036876470773})]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(user: int, N: int, model_variables: dict, remove_known_pos: bool = False) -> List[Tuple[int, Dict[str, float]]]:\n",
    "    predictions = get_prediction(user, model_variables, remove_known_pos=remove_known_pos)\n",
    "    recommendations = []\n",
    "    for item, pred in predictions.items():\n",
    "        add_item = (item, pred)\n",
    "        recommendations.append(add_item)\n",
    "        if len(recommendations) == N:\n",
    "            break\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = get_recommendations(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_items = get_relevant_items(data.test_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = relevant_items.keys()\n",
    "prec_at_N = dict.fromkeys(data.users)\n",
    "\n",
    "for user in users:\n",
    "    recommendations = get_recommendations(user, N, model_variables, remove_known_pos=True)\n",
    "    recommendations = [val[0] for val in recommendations]\n",
    "    hits = np.intersect1d(recommendations,\n",
    "                          relevant_items[user])\n",
    "    prec_at_N[user] = len(hits)/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([val for val in prec_at_N.values() if val is not None])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
