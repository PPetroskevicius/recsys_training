{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 9: LightFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You almost made it - this is the final lesson and it is also going to be the easiest one.\n",
    "\n",
    "As you may already assume - there are a lot of recommender packages in Python out there. In this lesson we will look at LightFM - an easy to use and lightweight implementation of different approaches and algorithms (FM, BPR, WARP, ...) to perform CF, CBF and hybrid recommenders.\n",
    "\n",
    "Within a few lines of code we set-up, train and use a recommender for recommendations.\n",
    "\n",
    "* [LightFM on GitHub](https://github.com/lyst/lightfm)\n",
    "* [LightFM documentation](https://making.lyst.com/lightfm/docs/home.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "from recsys_training.data import Dataset, genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightfm.datasets import fetch_movielens\n",
    "from lightfm.evaluation import precision_at_k\n",
    "from lightfm import LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml100k_ratings_filepath = '../../data/raw/ml-100k/u.data'\n",
    "ml100k_item_filepath = '../../data/raw/ml-100k/u.item'\n",
    "ml100k_user_filepath = '../../data/raw/ml-100k/u.user'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You may easily load Movielens Data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_movielens(min_rating=4.0, genre_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But, we want to use the exact same data and split that we used in the lessons before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset(ml100k_ratings_filepath)\n",
    "data.filter(min_rating=4.0)\n",
    "data.rating_split(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform our training and testing data into sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DataFrame to Train COO Matrix\n",
    "ratings = data.train_ratings[\"rating\"].values\n",
    "# We subtract 1 to make user/item ids 0-index-based\n",
    "rows = data.train_ratings[\"user\"].values - 1\n",
    "cols = data.train_ratings[\"item\"].values - 1\n",
    "\n",
    "train_mat = coo_matrix((ratings, (rows, cols)),\n",
    "                       shape=(data.n_users, data.n_items))\n",
    "\n",
    "\n",
    "# Test DataFrame to Test COO Matrix\n",
    "ratings = data.test_ratings[\"rating\"].values\n",
    "# We subtract 1 to make user/item ids 0-index-based\n",
    "rows = data.test_ratings[\"user\"].values - 1\n",
    "cols = data.test_ratings[\"item\"].values - 1\n",
    "\n",
    "test_mat = coo_matrix((ratings, (rows, cols)),\n",
    "                      shape=(data.n_users, data.n_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'no_components': 10,\n",
    "    'loss': 'bpr',\n",
    "    'learning_rate': 0.07,\n",
    "    'random_state': 42,\n",
    "    'user_alpha': 0.0002,\n",
    "    'item_alpha': 0.0002\n",
    "}\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_model = LightFM(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_model.fit(train_mat, epochs=epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the `MAP@10` on test data\n",
    "\n",
    "If we provide training data with evaluation, known positives will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_at_N = precision_at_k(cf_model, test_mat, train_mat, k=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_at_N.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the `MAP@10` on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_at_N = precision_at_k(cf_model, train_mat, k=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_at_N.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe try adding some regularization to improve the recommendation relevancy - simply add `user_alpha` and `item_alpha` to the `params` dictionary and find appropriate values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid (CF + CBF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load user and item features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(val, bounds):\n",
    "    min_max_range = bounds['max']-bounds['min']\n",
    "    return (val-bounds['min'])/min_max_range\n",
    "\n",
    "\n",
    "def user_profiler(group):\n",
    "    genre_dist = group[genres].mean()\n",
    "    year_dist = group['release_year'].describe()[['mean', 'std', '50%']]\n",
    "\n",
    "    return pd.concat((genre_dist, year_dist), axis=0)\n",
    "\n",
    "\n",
    "def get_user_profiles(ratings: pd.DataFrame,\n",
    "                      item_feat: pd.DataFrame,\n",
    "                      min_rating: float = 4.0) -> pd.DataFrame:\n",
    "    ratings = ratings[ratings.rating >= min_rating]\n",
    "    ratings = ratings[['user', 'item']]\n",
    "    ratings = ratings.merge(item_feat, on='item', how='left')\n",
    "    ratings.drop(['item'], axis=1, inplace=True)\n",
    "\n",
    "    grouped = ratings.groupby('user')\n",
    "    profiles = grouped.apply(user_profiler).reset_index()\n",
    "    profiles.rename(columns={'50%': 'median'}, inplace=True)\n",
    "    \n",
    "    return profiles\n",
    "\n",
    "\n",
    "item_feat = pd.read_csv(ml100k_item_filepath, sep='|', header=None,\n",
    "                        names=['item', 'title', 'release', 'video_release', 'imdb_url']+genres,\n",
    "                        engine='python')\n",
    "\n",
    "user_feat = pd.read_csv(ml100k_user_filepath, sep='|', header=None,\n",
    "                        names=['user', 'age', 'gender', 'occupation', 'zip'])\n",
    "\n",
    "# Infer the release year\n",
    "idxs = item_feat[item_feat['release'].notnull()].index\n",
    "item_feat.loc[idxs, 'release_year'] = item_feat.loc[idxs, 'release'].str.split('-')\n",
    "item_feat.loc[idxs, 'release_year'] = item_feat.loc[idxs, 'release_year'].apply(lambda val: val[2]).astype(int)\n",
    "\n",
    "# Impute median release year value for the items with missing release year\n",
    "top_year = item_feat.loc[idxs, 'release_year'].astype(int).describe()['50%']\n",
    "idx = item_feat[item_feat['release'].isnull()].index\n",
    "item_feat.loc[idx, 'release_year'] = top_year\n",
    "\n",
    "# Min-max scale the release year\n",
    "item_year_bounds = {'min': item_feat['release_year'].min(),\n",
    "                    'max': item_feat['release_year'].max()}\n",
    "item_feat['release_year'] = item_feat['release_year'].apply(\n",
    "    lambda year: min_max_scale(year, item_year_bounds))\n",
    "\n",
    "# Drop other columns\n",
    "item_feat.drop(['title', 'release', 'video_release', 'imdb_url'], axis=1, inplace=True)\n",
    "\n",
    "# Min-max scale the age\n",
    "user_age_bounds = {'min': user_feat['age'].min(),\n",
    "                   'max': user_feat['age'].max()}\n",
    "user_feat['age'] = user_feat['age'].apply(lambda age: min_max_scale(age, user_age_bounds))\n",
    "\n",
    "# Transform gender characters to numerical values (categories)\n",
    "genders = sorted(user_feat['gender'].unique())\n",
    "user_gender_map = dict(zip(genders, range(len(genders))))\n",
    "user_feat['gender'] = user_feat['gender'].map(user_gender_map)\n",
    "\n",
    "# Transform occupation strings to numerical values (categories)\n",
    "occupations = sorted(user_feat['occupation'].unique())\n",
    "user_occupation_map = dict(zip(occupations, range(len(occupations))))\n",
    "user_feat['occupation'] = user_feat['occupation'].map(user_occupation_map)\n",
    "\n",
    "# Transform the zip codes to categories keeping the first three digits and impute for missing\n",
    "idxs = user_feat[~user_feat['zip'].str.isnumeric()].index\n",
    "user_feat.loc[idxs, 'zip'] = '00000'\n",
    "zip_digits_to_cut = 3\n",
    "user_feat['zip'] = user_feat['zip'].apply(lambda val: int(val) // 10 ** zip_digits_to_cut)\n",
    "\n",
    "\n",
    "profiles = get_user_profiles(data.train_ratings, item_feat)\n",
    "user_feat = user_feat.merge(profiles, on='user', how='left')\n",
    "\n",
    "occupation_1H = pd.get_dummies(user_feat['occupation'], prefix='occupation')\n",
    "zip_1H = pd.get_dummies(user_feat['zip'], prefix='zip')\n",
    "\n",
    "user_feat.drop(['occupation', 'zip', ], axis=1, inplace=True)\n",
    "user_feat = pd.concat([user_feat, occupation_1H, zip_1H], axis=1)\n",
    "\n",
    "user_feat.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "user_feat.index = user_feat['user'].values\n",
    "user_feat.drop('user', axis=1, inplace=True)\n",
    "\n",
    "item_feat.index = item_feat['item'].values\n",
    "item_feat.drop('item', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(user_feat==0).sum().sum()/user_feat.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(item_feat==0).sum().sum()/item_feat.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create User Feature COO Matrix\n",
    "# user_feat_mat = coo_matrix(np.eye(data.n_users))\n",
    "user_feat_mat = coo_matrix(np.concatenate((user_feat.values, np.eye(data.n_users)), axis=1))\n",
    "\n",
    "# Create Item Feature COO Matrix\n",
    "# item_feat_mat = coo_matrix(np.eye(data.n_items))\n",
    "item_feat_mat = coo_matrix(np.concatenate((item_feat.values, np.eye(data.n_items)), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feat_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feat_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Parrot.png)\n",
    "\n",
    "**Task:** Check the [lightFM API](https://making.lyst.com/lightfm/docs/home.html) to see how you can incorporate proper data - can you tweek the algorithm to beat pure Collaborative Filtering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'no_components': 10,\n",
    "    'loss': 'warp',\n",
    "    'learning_rate': 0.03,\n",
    "    'random_state': 42,\n",
    "    'user_alpha': 0.0001,\n",
    "    'item_alpha': 0.0001\n",
    "}\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_model = None\n",
    "\n",
    "#\n",
    "# Up to you ;)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_at_N = precision_at_k(hybrid_model,\n",
    "                           test_mat,\n",
    "                           train_mat,\n",
    "                           k=N,\n",
    "                           user_features=user_feat_mat,\n",
    "                           item_features=item_feat_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_at_N.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
